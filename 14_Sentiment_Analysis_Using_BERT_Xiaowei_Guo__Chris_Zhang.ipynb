{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaowei-v/HW4-/blob/main/14_Sentiment_Analysis_Using_BERT_Xiaowei_Guo__Chris_Zhang.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " <div align=right>\n",
        "Xiaowei Guo<br>\n",
        "Chris Zhang<br>\n",
        "</div>\n",
        "\n",
        " \n",
        " <font size=\"9\">**Sentiment Analysis:**</font> \n",
        "\n",
        " <font size=\"9\">**BERT Model, Huggingface and Application**</font> \n",
        "\n",
        "<br/><br/> \n",
        "\n",
        "# Learning Objectives\n",
        "The purpose of this lecture is a brief introduction to some basics of the BERT model (bidirectional encoder representations from transformers) and its application in sentiment analysis using pre-trained models. We use FinBERT and the pretrained model as an example to carry out sentiment analysis on financial news headlines datasets. But you are free to explore models in other domains for different purposes.\n",
        "\n",
        "###  Follow These Steps:\n",
        "\n",
        "\n",
        "*   Fetch all the materials\n",
        "*   Open the link navigating to google cola\n",
        "*   Click on Files on the left and upload the dataset csv file which is used for sentiment analysis\n",
        "\n",
        "\n",
        "### 1. Basic Introduction to BERT and Transformer Library\n",
        "\n",
        "*   What is BERT:\n",
        "\n",
        "\n",
        "> We introduce a new **language representation model** called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the **pre-trained** BERT model can be **fine-tuned with just one additional output layer** to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. Refer to the original article for further information: https://arxiv.org/abs/1810.04805\n",
        "\n",
        "*   Getting familiar with transformers and pipeline\n",
        "\n",
        "\n",
        "> Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. \n",
        "\n",
        "> basic logic of the pipline: \n",
        "raw text -> representation -> pre-trained model -> output layer according to specific task\n",
        "\n",
        "\n",
        "### 2. Application: Sentiment Analysis on Financial News Dataset\n",
        "\n",
        "\n",
        "*   Pre-processing\n",
        "*   Sentiment analysis following the pipline\n"
      ],
      "metadata": {
        "id": "kvcfsKA71nbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Libraries"
      ],
      "metadata": {
        "id": "waIQbGxhJuMZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1VjWKsgO82b"
      },
      "outputs": [],
      "source": [
        "# Run the following command to install required packages\n",
        "! pip install transformers \n",
        "! pip install torch torchvision\n",
        "! pip install pysentiment2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all the libraries\n",
        "import transformers \n",
        "print(transformers.__version__)\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "HU6PvmWZQ2aH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction to basic operations**\n",
        "\n",
        "*  step1: Preprocessing (Tokenization) input data \n",
        "*  step2: Import the trained model \n",
        "*  step3: Input the data to make prediction  \n",
        "\n"
      ],
      "metadata": {
        "id": "npIbCsCICdpX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------"
      ],
      "metadata": {
        "id": "651WrPNxSZuw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tokenization"
      ],
      "metadata": {
        "id": "ekkbhCxkEwMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Although the result is relatively good, it is not satisfactory.\""
      ],
      "metadata": {
        "id": "vd56rOEGHHiQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# created tokenizer based on the pre-trained model we will use \n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "RHTDYCMrEvnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> the tokenizer we used is relativel small, it only 240k, but the pre-trained model is really huge, we'll see it later <b>"
      ],
      "metadata": {
        "id": "bfewWyTAQ_hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokennization for the text and check the keys in the result \n",
        "tokenize_sample_text = tokenizer(sample_text, return_tensors = 'pt')\n",
        "print(tokenize_sample_text.keys())"
      ],
      "metadata": {
        "id": "2S9OB_-KFHHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the full tokenization \n",
        "tokenize_sample_text = tokenizer(sample_text)\n",
        "token_id_asinput = tokenize_sample_text.get('input_ids')\n",
        "print('input format: ', tokenize_sample_text, '\\n')\n",
        "print('tokenized text: ',[tokenizer.ids_to_tokens[x] for x in tokenize_sample_text['input_ids']],'\\n')\n",
        "print('token id: ', token_id_asinput)"
      ],
      "metadata": {
        "id": "A7vnnizRIGOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**input ids:** the id that assigned to each tokens, not vector, just a id \\\n",
        "**attention_mask:** 1 is the actual token, 0 stands for the padding token that created by the model "
      ],
      "metadata": {
        "id": "i668Z6OERnPr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------"
      ],
      "metadata": {
        "id": "tbv5gvbuSqov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Output Probability\n",
        "\n"
      ],
      "metadata": {
        "id": "9B-TanrEQz2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the model \n",
        "classifier_sample = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "LxqIL-ZVSVth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<b> the model that we imported is really huge, around 440MB which unable to load in local jupyternotebook, therefore we use colab <b>"
      ],
      "metadata": {
        "id": "478GtFw7Qn42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pass the input into the classifier\n",
        "tokenize_sample_text = tokenizer(sample_text, return_tensors = 'pt')\n",
        "return_result = classifier_sample(**tokenize_sample_text)\n",
        "\n",
        "# get the logits score which is the raw score for each category before normalization\n",
        "logit_sample = return_result.logits\n",
        "\n",
        "print('result formate: ', return_result)\n",
        "print('logit part: ', logit_sample)"
      ],
      "metadata": {
        "id": "5xVsjBQWSn7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is logits:\n",
        "\n",
        "> The vector of raw (non-normalized) predictions that the last layer of the model generates, which is ordinarily then passed to a normalization function. If the model is solving a multi-class classification problem, logits typically become an input to the softmax function. The softmax function then generates a vector of (normalized) probabilities with one value for each possible class.\n",
        "\n"
      ],
      "metadata": {
        "id": "NPhsdt8K-Aus"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b> for more information about .forward() function's result, including loss, grad_fn and etc.,<b> https://huggingface.co/transformers/v3.0.2/model_doc/bert.html"
      ],
      "metadata": {
        "id": "mnSSEym5QSnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the softmax function to normalize the raw score \n",
        "# returns the probability for each label\n",
        "pred_sample = F.softmax(logit_sample, dim = -1)\n",
        "pred_sample "
      ],
      "metadata": {
        "id": "cxiEXAqLTFxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For details in model output: https://huggingface.co/docs/transformers/main_classes/output"
      ],
      "metadata": {
        "id": "hknFxrCt626r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Sentiment Analysis Example: Finanical News sentiment Analysis**"
      ],
      "metadata": {
        "id": "FtucaEIIXPVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part we practice application of BERT model in sentiment analysis on a financial news dataset from Kaggle. Because of the time limit, we are using only part of the original dataset. You may download the original dataset and check the features [here](https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests).\n",
        "\n",
        "In order to build our model with application in specific domain (finance), here we use the pre-trained model FinBERT, which is a financial domain-specific pre-trained language model based on BERT, trained on 4.9 billion financial texts. The goal is to enhance financial NLP research and practice. You may find detailed information and tutorials [here](https://github.com/yya518/FinBERT).\n",
        "\n",
        "This is a simple example of domain-specific pre-trained model. You are free to explore other models with application in other fields. Many of such models and corresponding datasets can be found on huggingface official website: https://huggingface.co/models."
      ],
      "metadata": {
        "id": "MPr5Y-TbYGJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our project follows these steps:\n",
        "\n",
        "\n",
        "1.   Cleaning the dataset (drop null etc.)\n",
        "2.   Import FinBERT Model\n",
        "3.   Pipeline and function for sentiment detaction\n",
        "4.   Apply it to the dataframe"
      ],
      "metadata": {
        "id": "gxpQXpjdZ5PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____________________________________________________________________________________________"
      ],
      "metadata": {
        "id": "MfZMnHLefPQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: load in dataset and preprocessing**"
      ],
      "metadata": {
        "id": "Kd7u-IHEdQEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load in dataset as pandas dataframe\n",
        "df = pd.read_csv('/content/analyst_ratings_processed.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "F76RnpymkHEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check na values\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "3bmGLgv1lXNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check data shape\n",
        "df.shape\n",
        "\n",
        "# we drop all the null values directly because we have a rather large sample size\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "zg1eOKdnkbra"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check null values again\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "bnOW1gq6kdrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Launch the FinBERT Model to Implement Tokenization**\n",
        "\n",
        "The FinBERT model consists of two modules:\n",
        "* BertTokenizer: tokenize the raw text input into word tokens\n",
        "* BertForSequenceClassification: The FinBERT forward model to putput the label probability \n"
      ],
      "metadata": {
        "id": "kXEOEZzImA9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download/load the pretrained/fine-tuned model weights and instantiate the classifier for this task"
      ],
      "metadata": {
        "id": "dgLgRAQQ48N7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the directory of the pre-trained model we use\n",
        "model_dir = 'yiyanghkust/finbert-tone'\n",
        "token_dir = 'yiyanghkust/finbert-tone'"
      ],
      "metadata": {
        "id": "gA8_chQeZFZG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load the tokenizer(FinVocab)\n",
        "finBERT_tokenizer = BertTokenizer.from_pretrained(token_dir)\n",
        "\n",
        "#load the FinBERT model weight \n",
        "fin_Bert_engine = BertForSequenceClassification.from_pretrained(model_dir, num_labels = 3)"
      ],
      "metadata": {
        "id": "i1tXgxuwZcA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Sentiment Classification**\n"
      ],
      "metadata": {
        "id": "BmGcUpwZbLH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use pipline abstraction to apply the pre-traied model for specific tasks\n",
        "\n",
        "> The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the task summary for examples of use. Refer to https://huggingface.co/docs/transformers/main_classes/pipelines for details.\n",
        "\n"
      ],
      "metadata": {
        "id": "uVIGqHT_32n5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "* Example Input: \"We also believe that there's generally way too much optimism in Techland with a recession very likely to hit next year and many of out favorite forward Tech spending indicators already heading south.\""
      ],
      "metadata": {
        "id": "dzq_bwXJ4DGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the pipline function to go through the previous pipline\n",
        "# initialize the pipline: specify the task, the model and the tokenizer\n",
        "nlp = pipeline(\"sentiment-analysis\", model=fin_Bert_engine, tokenizer=finBERT_tokenizer)\n",
        "\n",
        "# the input raw text\n",
        "sentences = ['''\n",
        "We also believe that there's generally way too much optimism in Techland \n",
        "with a recession very likely to hit next year and many of out favorite \n",
        "forward Tech spending indicators already heading south.''']\n",
        "\n",
        "# output a dictionary of labels and scores\n",
        "results = nlp(sentences)\n",
        "print(results)  #LABEL_0: neutral; LABEL_1: positive; LABEL_2: negative"
      ],
      "metadata": {
        "id": "J0dw194C7F3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to carry out sentiment analysis using pipline\n",
        "def SentimentAnalyzer_pipe(doc):\n",
        "  '''\n",
        "  Feed the input text to the model and get the classification for the input text\n",
        "  Input:\n",
        "       a string: not been processed \n",
        "  Returns the corresponding label\n",
        "  '''\n",
        "  nlp = pipeline(\"sentiment-analysis\", model=fin_Bert_engine, tokenizer=finBERT_tokenizer)\n",
        "  results = nlp(doc)\n",
        "  return results[0]['label']\n",
        "  "
      ],
      "metadata": {
        "id": "kO4TUplir09y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: apply the funtion to the dataframe to label each instance**"
      ],
      "metadata": {
        "id": "XNP9IcVAs4EU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use pandas apply method to apply the sentiment analyzer function to the 'title' column\n",
        "# label each row with the sentiment by adding the 'label' column\n",
        "df['label'] = df['title'].apply(SentimentAnalyzer_pipe)"
      ],
      "metadata": {
        "id": "-ybbN_cHs3Yn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the labels the function generates\n",
        "df"
      ],
      "metadata": {
        "id": "axFdsWkXuqkf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "oyb7fXGAvHRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Resources\n",
        "We only provide some basic logic of how to use BERT model to conduct sentiment classification in today's lecture, leaving out all the details about the pretraining, fine-tuning and the detailed operation of each layer of the neuro-network. We are not able to explain the fundamental logics in detail due to time limit.\n",
        "\n",
        "However, if you are interested in learning the theory and math behind the model or the detailed methods and the process of how BERT model is pretrained and fine-tuned for different tasks, you can refer to the textbook [here](https://web.stanford.edu/~jurafsky/slp3/11.pdf)\n",
        "\n",
        "The resources that we mentioned during the lecture are listed here:\n",
        "\n",
        "\n",
        "*   BERT original article: https://arxiv.org/abs/1810.04805\n",
        "*   Huggingface offical website: https://huggingface.co/models\n",
        "*   Financial news dataset: https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests\n",
        "*   FinBERT documentation: https://github.com/yya518/FinBERT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "trIly8FCtzLg"
      }
    }
  ]
}